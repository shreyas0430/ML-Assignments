{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load doc into memory\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, 'r')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "# To break into sentences from text\n",
    "def sentence(text):\n",
    "    sentences = []\n",
    "    sentences = list(text.split(\"\\n\"))\n",
    "    return sentences\n",
    "    \n",
    "\n",
    "# Load the document\n",
    "filename = 'a1_data/a1_d3.txt'\n",
    "text = load_doc(filename)\n",
    "sentences = sentence(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text preprocessing\n",
    "lower_case_sentences = []\n",
    "for i in sentences:\n",
    "    lower_case_sentences.append(i.lower())\n",
    "\n",
    "no_punctuations = []\n",
    "for i in lower_case_sentences:\n",
    "    no_punctuations.append(''.join(c for c in i if c not in string.punctuation))\n",
    "\n",
    "clean_data = []\n",
    "for i in no_punctuations:\n",
    "    sub = i.split(', ')\n",
    "    sub1 = sub[0].split('\\t')\n",
    "    clean_data.append(sub1)\n",
    "clean_data.remove(clean_data[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                Review Sentiment\n",
      "0    so there is no way for me to plug it in here i...         0\n",
      "1                            good case excellent value         1\n",
      "2                                great for the jawbone         1\n",
      "3    tied to charger for conversations lasting more...         0\n",
      "4                                     the mic is great         1\n",
      "..                                                 ...       ...\n",
      "995  the screen does get smudged easily because it ...         0\n",
      "996  what a piece of junk i lose more calls on this...         0\n",
      "997                        item does not match picture         0\n",
      "998  the only thing that disappoint me is the infra...         0\n",
      "999  you can not answer calls with the unit never w...         0\n",
      "\n",
      "[1000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(clean_data, columns =['Review', 'Sentiment'])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy\n",
      "80.4   ±   0.9695359714832659\n",
      "\n",
      "F-score\n",
      "0.8151240521041979   ±   0.01629990354617741\n"
     ]
    }
   ],
   "source": [
    "# Creating a list to store the F-score and accuracy for each fold iterations\n",
    "accu = []\n",
    "score = []\n",
    "count_total = 0\n",
    "\n",
    "# Split a dataset into 5 folds\n",
    "def cross_validation(df, n_folds):\n",
    "    df_split = list()\n",
    "    df_copy = list(df)\n",
    "    fold_size = int(len(df) / 5)\n",
    "    for i in range (n_folds):\n",
    "        fold = []\n",
    "        while len(fold) < fold_size:\n",
    "            index = random.randrange(0,len(df_copy))\n",
    "            fold.append(df_copy.pop(index))\n",
    "        df_split.append(fold)\n",
    "    return df_split\n",
    "\n",
    "# Setting the model's vocabulary\n",
    "def vocab_freq(train_df):\n",
    "    train_sentences = train_df['Review'].values\n",
    "    train_sentences_list = train_sentences.tolist()\n",
    "    all_words_train = []\n",
    "    for i in train_sentences_list:\n",
    "        all_words_train.extend(i.split(' '))\n",
    "    vocab,count = np.unique(np.array(all_words_train),return_counts=True)\n",
    "    return (vocab,count)\n",
    "\n",
    "# Calculating likelihood probability P(d|C)\n",
    "# Writing a function for a given class C = 1,0\n",
    "def posterior_prob(train_df,vocab,count,words_test,prob_class, class_count):\n",
    "    posterior_prob = list()\n",
    "    #Calculations for test data in row i\n",
    "    for i in words_test:\n",
    "        likelihood_prob = 1\n",
    "        word_test_array = np.array(i)\n",
    "        vocab_test,count_test = np.unique(word_test_array,return_counts=True)\n",
    "        #j returns the elements of the iterable list i\n",
    "        for j in i:\n",
    "            try:\n",
    "                index = list(vocab).index(j)\n",
    "                # Here likelihood probability is returned for the ith row of test data\n",
    "                likelihood_prob *= ((count[index] + 1) / (np.sum(count) + np.sum(count_total) + 1))\n",
    "            except ValueError:\n",
    "                likelihood_prob *= ((0 + 1) / (np.sum(count) + np.sum(count_total) + 1))\n",
    "            \n",
    "        # Return the probability P(d|C)*P(C)\n",
    "        posterior = prob_class*likelihood_prob\n",
    "        posterior_prob.append(posterior)\n",
    "    return posterior_prob\n",
    "\n",
    "# implementing f1 score per test\n",
    "def f1_score_calc(y_actual, y_pred):\n",
    "    y_actual = np.array(y_actual.astype(int))\n",
    "    y_pred = np.array(y_pred.astype(int))\n",
    "    tp = 0\n",
    "    fn = 0\n",
    "    fp = 0\n",
    "    for i in range(len(y_actual)):\n",
    "        if (y_actual[i] == 1) and (y_pred[i] == 1):\n",
    "            tp += 1\n",
    "        if (y_actual[i] == 0) and (y_pred[i] == 1):\n",
    "            fp += 1\n",
    "        if (y_actual[i] == 1) and (y_pred[i] == 0):\n",
    "            fn += 1\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    return ( 2 * precision * recall / (precision + recall) )\n",
    "\n",
    "def accuracy_metric(actual, predicted):\n",
    "    correct = 0\n",
    "    for i in range(len(actual)):\n",
    "        if actual[i] == predicted[i]:\n",
    "            correct += 1\n",
    "    return correct / float(len(actual)) * 100.0\n",
    "\n",
    "def Naive_Bayes(train_set,test_set):\n",
    "    train_df = pd.DataFrame(train_set, columns =['Review', 'Sentiment'])\n",
    "    test_df = pd.DataFrame(test_set,columns =['Review', 'Sentiment'])\n",
    "    train_df_positive = train_df.loc[train_df['Sentiment']=='1']\n",
    "    train_df_negative = train_df.loc[train_df['Sentiment']=='0']\n",
    "\n",
    "\n",
    "    #Setting the positive sentiment and negative sentiment vocab and frequency\n",
    "    vocab_positive, count_positive = vocab_freq(train_df_positive)\n",
    "    vocab_negative, count_negative = vocab_freq(train_df_negative)\n",
    "    vocab_total, count_total = vocab_freq(train_df)\n",
    "\n",
    "\n",
    "    #Gives the probability P(C) or prior probability\n",
    "    # no. of sentiment values is the same as the no. of reviews in train_set\n",
    "    train_sentiments = train_df['Sentiment'].values\n",
    "    sentiment,count = np.unique(train_sentiments,return_counts = True)\n",
    "\n",
    "    positive_review_count = count[1]\n",
    "    negative_review_count = count[0]\n",
    "\n",
    "    prob_positive = positive_review_count / (positive_review_count + negative_review_count)\n",
    "    prob_negative = negative_review_count / (positive_review_count + negative_review_count)\n",
    "\n",
    "    # extracting the words from the test_set\n",
    "    test_sentences = test_df['Review'].values\n",
    "    test_sentences_list = test_sentences.tolist()\n",
    "    words_test = []\n",
    "    for i in test_sentences_list:\n",
    "        words_test.append(i.split(' '))\n",
    "\n",
    "\n",
    "            \n",
    "    posterior_prob_positive = posterior_prob(train_df,vocab_positive,count_positive,words_test,prob_positive,positive_review_count)\n",
    "    posterior_prob_negative = posterior_prob(train_df,vocab_negative,count_negative,words_test,prob_negative,negative_review_count)\n",
    "\n",
    "    test_predict = list()\n",
    "\n",
    "    #predict the Sentiment\n",
    "    for i in range (len(test_set)):\n",
    "        if posterior_prob_positive[i] > posterior_prob_negative[i]:\n",
    "            test_predict.append(\"1\")\n",
    "        else:\n",
    "            test_predict.append(\"0\")\n",
    "        \n",
    "    test_df['Predicted Sentiment'] = test_predict \n",
    "    f1_score = f1_score_calc(test_df['Sentiment'], test_df['Predicted Sentiment'])\n",
    "    score.append(f1_score)\n",
    "    accuracy = accuracy_metric(test_df['Sentiment'], test_df['Predicted Sentiment'])\n",
    "    accu.append(accuracy)\n",
    "\n",
    "\n",
    "\n",
    "folds = cross_validation(clean_data, 5)\n",
    "for fold in folds:\n",
    "    train_set = list(folds)\n",
    "    train_set.remove(fold)\n",
    "    train_set = sum(train_set, [])\n",
    "    test_set = list()\n",
    "    for row in fold:\n",
    "        row_copy = list(row)\n",
    "        test_set.append(row_copy)\n",
    "    Naive_Bayes(train_set, test_set)\n",
    "        \n",
    "accuracy = np.array(accu)\n",
    "f_score = np.array(score)\n",
    "print('\\nAccuracy')\n",
    "print(np.mean(accuracy),' ',u'\\u00b1',' ',np.std(accuracy))\n",
    "print('\\nF-score')\n",
    "print(np.mean(f_score),' ',u'\\u00b1',' ',np.std(f_score))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
